{"cells":[{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["<a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01\"><img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/Logos/organization_logo/organization_logo.png\" width = 400> </a>\n","\n","<h1 align=center><font size = 5>Pre-Trained Models</font></h1>\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["## Objective\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["## Table of Contents\n","\n","<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n","\n","<font size = 3> \n","\n","1.  <a href=\"https://#item31\">Import Libraries and Packages</a>\n","2.  <a href=\"https://#item32\">Download Data</a>\n","3.  <a href=\"https://#item33\">Define Global Constants</a>\n","4.  <a href=\"https://#item34\">Construct ImageDataGenerator Instances</a>\n","5.  <a href=\"https://#item35\">Compile and Fit Model</a>\n","\n","</font>\n","\n","</div>\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":[]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["<a id='item31'></a>\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["## Import Libraries and Packages\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["Let's start the lab by importing the libraries that we will be using in this lab.\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from keras.preprocessing.image import ImageDataGenerator"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library.\n"]},{"cell_type":"code","execution_count":2,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["import keras\n","from keras.models import Sequential\n","from keras.layers import Dense"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"]},{"cell_type":"code","execution_count":3,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["from keras.applications import ResNet50\n","from keras.applications.resnet import preprocess_input, ResNet50"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["<a id='item32'></a>\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["## Download Data\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["For your convenience, I have placed the data on a server which you can retrieve easily using the **wget** command. So let's run the following line of code to get the data. Given the large size of the image dataset, it might take some time depending on your internet speed.\n"]},{"cell_type":"code","execution_count":4,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["## get the data\n","#!wget https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0321EN/data/concrete_data_week3.zip"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["And now if you check the left directory pane, you should see the zipped file *concrete_data_week3.zip* appear. So, let's go ahead and unzip the file to access the images. Given the large number of images in the dataset, this might take a couple of minutes, so please be patient, and wait until the code finishes running.\n"]},{"cell_type":"code","execution_count":5,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false},"scrolled":true},"outputs":[],"source":["#!unzip concrete_data_week3.zip"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks.\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50**\\* error. So please **DO NOT DO IT**.\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["<a id='item33'></a>\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["## Define Global Constants\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["Here, we will define constants that we will be using throughout the rest of the lab.\n","\n","1.  We are obviously dealing with two classes, so *num_classes* is 2.\n","2.  The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n","3.  We will training and validating the model using batches of 100 images.\n"]},{"cell_type":"code","execution_count":6,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["num_classes = 2\n","\n","image_resize = 224\n","\n","batch_size_training = 100\n","batch_size_validation = 100"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["<a id='item34'></a>\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["## Construct ImageDataGenerator Instances\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"]},{"cell_type":"code","execution_count":7,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["data_generator = ImageDataGenerator(\n","    preprocessing_function=preprocess_input,\n",")"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["Next, we will use the *flow_from_directory* method to get the training images as follows:\n"]},{"cell_type":"code","execution_count":8,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 30001 images belonging to 2 classes.\n"]}],"source":["train_generator = data_generator.flow_from_directory(\n","    'concrete_data_week3/concrete_data_week3/train',\n","    target_size=(image_resize, image_resize),\n","    batch_size=batch_size_training,\n","    class_mode='categorical')"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"]},{"cell_type":"code","execution_count":21,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 10001 images belonging to 2 classes.\n","Found 10001 images belonging to 2 classes.\n"]}],"source":["## Type your answer here\n","\n","validation_generator = data_generator.flow_from_directory(\n","    'concrete_data_week3/concrete_data_week3/valid',\n","    target_size=(image_resize, image_resize),\n","    batch_size=batch_size_training,\n","    class_mode='categorical')\n","\n","testing_generator = data_generator.flow_from_directory(\n","    'concrete_data_week3/concrete_data_week3/valid',\n","    target_size=(image_resize, image_resize),\n","    batch_size=batch_size_training,\n","    class_mode='categorical')\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["Double-click **here** for the solution.\n","\n","<!-- The correct answer is:\n","validation_generator = data_generator.flow_from_directory(\n","    'concrete_data_week3/valid',\n","    target_size=(image_resize, image_resize),\n","    batch_size=batch_size_validation,\n","    class_mode='categorical')\n","-->\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["<a id='item35'></a>\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["## Build, Compile and Fit Model\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["In this section, we will start building our model. We will use the Sequential model class from Keras.\n"]},{"cell_type":"code","execution_count":10,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["model = Sequential()"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"]},{"cell_type":"code","execution_count":11,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["model.add(ResNet50(\n","    include_top=False,\n","    pooling='avg',\n","    weights='imagenet',\n","    ))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"]},{"cell_type":"code","execution_count":12,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["model.add(Dense(num_classes, activation='softmax'))"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["You can access the model's layers using the *layers* attribute of our model object.\n"]},{"cell_type":"code","execution_count":13,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[{"data":{"text/plain":["[<keras.engine.functional.Functional at 0x2938f13ad60>,\n"," <keras.layers.core.dense.Dense at 0x2938e35ac40>]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["model.layers"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["You can access the ResNet50 layers by running the following:\n"]},{"cell_type":"code","execution_count":14,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false},"scrolled":true},"outputs":[{"data":{"text/plain":["[<keras.engine.input_layer.InputLayer at 0x2938e3b0f70>,\n"," <keras.layers.reshaping.zero_padding2d.ZeroPadding2D at 0x2938ed5b820>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938ed5bac0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938eddeaf0>,\n"," <keras.layers.core.activation.Activation at 0x2938ed5baf0>,\n"," <keras.layers.reshaping.zero_padding2d.ZeroPadding2D at 0x2938f06cac0>,\n"," <keras.layers.pooling.max_pooling2d.MaxPooling2D at 0x2938f09fa30>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938f0a8310>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938f0afdf0>,\n"," <keras.layers.core.activation.Activation at 0x2938f09fb20>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938f0cbbe0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938f0d5c70>,\n"," <keras.layers.core.activation.Activation at 0x2938f0d5130>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938f09f970>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938f0cb250>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938f0a81f0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938f0e8cd0>,\n"," <keras.layers.merging.add.Add at 0x2938f0d26d0>,\n"," <keras.layers.core.activation.Activation at 0x2938f0f55e0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938f0effd0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938f106a90>,\n"," <keras.layers.core.activation.Activation at 0x2938f106d60>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938f1136d0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938f126400>,\n"," <keras.layers.core.activation.Activation at 0x2938f11e3d0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938f1165b0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938f12e820>,\n"," <keras.layers.merging.add.Add at 0x2938f13e100>,\n"," <keras.layers.core.activation.Activation at 0x2938f13ef70>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938f12e970>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938f13abe0>,\n"," <keras.layers.core.activation.Activation at 0x2938f135400>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938f10cac0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938f08e340>,\n"," <keras.layers.core.activation.Activation at 0x2938f0ef7c0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938f14bfd0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938f1060d0>,\n"," <keras.layers.merging.add.Add at 0x2938f0e2730>,\n"," <keras.layers.core.activation.Activation at 0x2938f152d90>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938f157af0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938f1653d0>,\n"," <keras.layers.core.activation.Activation at 0x2938f09fe20>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938f17fa30>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938f177280>,\n"," <keras.layers.core.activation.Activation at 0x2938f17ebe0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938f157910>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938f5a4a30>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938f150130>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938f5ab2e0>,\n"," <keras.layers.merging.add.Add at 0x2938f18e460>,\n"," <keras.layers.core.activation.Activation at 0x2938f5c8250>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938f5c8880>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938f5c8610>,\n"," <keras.layers.core.activation.Activation at 0x2938f5a4370>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938f18e9a0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938f18ed60>,\n"," <keras.layers.core.activation.Activation at 0x2938f17fdf0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938f165100>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938f113940>,\n"," <keras.layers.merging.add.Add at 0x2938f0af550>,\n"," <keras.layers.core.activation.Activation at 0x2938f0c5940>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938f5d7970>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938f5d7430>,\n"," <keras.layers.core.activation.Activation at 0x2938f5cfee0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938f5db9a0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938f5dbf10>,\n"," <keras.layers.core.activation.Activation at 0x2938f5da4c0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938f5f1fd0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938f5ff160>,\n"," <keras.layers.merging.add.Add at 0x2938f5ff1c0>,\n"," <keras.layers.core.activation.Activation at 0x2938f5f1fa0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938f615cd0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938f615370>,\n"," <keras.layers.core.activation.Activation at 0x2938f607370>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938f61cf70>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938fe3fe80>,\n"," <keras.layers.core.activation.Activation at 0x2938fe40580>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938f60e4f0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938f5f9700>,\n"," <keras.layers.merging.add.Add at 0x2938f607730>,\n"," <keras.layers.core.activation.Activation at 0x2938f152d60>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938f10c7c0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938f1777c0>,\n"," <keras.layers.core.activation.Activation at 0x2938f172c40>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938fe51970>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938fe4cb50>,\n"," <keras.layers.core.activation.Activation at 0x2938fe518e0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938f5ae5e0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938fe69370>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938f10cee0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x29390e81b50>,\n"," <keras.layers.merging.add.Add at 0x29390e811c0>,\n"," <keras.layers.core.activation.Activation at 0x2938fe69eb0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x29390e95730>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x29390e95d30>,\n"," <keras.layers.core.activation.Activation at 0x29390e88a90>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x29390ea2a30>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x29390ea29a0>,\n"," <keras.layers.core.activation.Activation at 0x29390eb7310>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x29390eb7610>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x29390eaf2b0>,\n"," <keras.layers.merging.add.Add at 0x29390eaf6a0>,\n"," <keras.layers.core.activation.Activation at 0x2938fe60c70>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938f165df0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938fe4fd90>,\n"," <keras.layers.core.activation.Activation at 0x29390ea7e50>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938f5ea340>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938f5f19a0>,\n"," <keras.layers.core.activation.Activation at 0x2938f5ea1c0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x29390ed4280>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x29390ec5af0>,\n"," <keras.layers.merging.add.Add at 0x29390ec9d90>,\n"," <keras.layers.core.activation.Activation at 0x29390eddc70>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x29390ed4e50>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x29390edda60>,\n"," <keras.layers.core.activation.Activation at 0x29390ed4dc0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x29390ee8940>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x29390f0cc10>,\n"," <keras.layers.core.activation.Activation at 0x29390ef2bb0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x29390f1ac70>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x29390f1a4f0>,\n"," <keras.layers.merging.add.Add at 0x29390f20370>,\n"," <keras.layers.core.activation.Activation at 0x29390f20fd0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x29390f28c10>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x29390f383d0>,\n"," <keras.layers.core.activation.Activation at 0x29390f38280>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x29390f11c40>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x29390ef6250>,\n"," <keras.layers.core.activation.Activation at 0x29390f2f490>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x29390ec3f40>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x29390e81f10>,\n"," <keras.layers.merging.add.Add at 0x29390ef20d0>,\n"," <keras.layers.core.activation.Activation at 0x29390f32940>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x29390ee80d0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x29393063700>,\n"," <keras.layers.core.activation.Activation at 0x29390e81a00>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x29393061cd0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x293930819d0>,\n"," <keras.layers.core.activation.Activation at 0x29393061910>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2939308d520>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2939308d250>,\n"," <keras.layers.merging.add.Add at 0x29393078670>,\n"," <keras.layers.core.activation.Activation at 0x29393091a90>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x29393091a00>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x293930af3a0>,\n"," <keras.layers.core.activation.Activation at 0x293930bc0a0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x293930c6400>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938ed5bf70>,\n"," <keras.layers.core.activation.Activation at 0x29381f61c70>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2939308dbe0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938f0d2e80>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x293930911f0>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938f5dbe20>,\n"," <keras.layers.merging.add.Add at 0x2938f0e8250>,\n"," <keras.layers.core.activation.Activation at 0x2938f5bcca0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938f172640>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938f177d30>,\n"," <keras.layers.core.activation.Activation at 0x29390e88130>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938fe51430>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x29390e81fa0>,\n"," <keras.layers.core.activation.Activation at 0x2938f177f40>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x293930a0430>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x29390ea7f40>,\n"," <keras.layers.merging.add.Add at 0x293930a0100>,\n"," <keras.layers.core.activation.Activation at 0x293930b8190>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938f618b50>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938f61ca00>,\n"," <keras.layers.core.activation.Activation at 0x2938f60ecd0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2938f11e370>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x2938fe40850>,\n"," <keras.layers.core.activation.Activation at 0x2938f61c3d0>,\n"," <keras.layers.convolutional.conv2d.Conv2D at 0x2939308a910>,\n"," <keras.layers.normalization.batch_normalization.BatchNormalization at 0x29393061460>,\n"," <keras.layers.merging.add.Add at 0x2938fe4f9a0>,\n"," <keras.layers.core.activation.Activation at 0x29393067eb0>,\n"," <keras.layers.pooling.global_average_pooling2d.GlobalAveragePooling2D at 0x2938f06c550>]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["model.layers[0].layers"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"]},{"cell_type":"code","execution_count":15,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["model.layers[0].trainable = False"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"]},{"cell_type":"code","execution_count":16,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," resnet50 (Functional)       (None, 2048)              23587712  \n","                                                                 \n"," dense (Dense)               (None, 2)                 4098      \n","                                                                 \n","=================================================================\n","Total params: 23,591,810\n","Trainable params: 4,098\n","Non-trainable params: 23,587,712\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["Next we compile our model using the **adam** optimizer.\n"]},{"cell_type":"code","execution_count":17,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"]},{"cell_type":"code","execution_count":18,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["steps_per_epoch_training = len(train_generator)\n","steps_per_epoch_validation = len(validation_generator)\n","num_epochs = 2"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"]},{"cell_type":"code","execution_count":19,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\vadim\\AppData\\Local\\Temp\\ipykernel_1232\\251737888.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n","  fit_history = model.fit_generator(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/2\n","301/301 [==============================] - 2581s 9s/step - loss: 0.0254 - accuracy: 0.9926 - val_loss: 0.0068 - val_accuracy: 0.9981\n","Epoch 2/2\n","301/301 [==============================] - 2478s 8s/step - loss: 0.0054 - accuracy: 0.9987 - val_loss: 0.0048 - val_accuracy: 0.9986\n"]}],"source":["fit_history = model.fit_generator(\n","    train_generator,\n","    steps_per_epoch=steps_per_epoch_training,\n","    epochs=num_epochs,\n","    validation_data=validation_generator,\n","    validation_steps=steps_per_epoch_validation,\n","    verbose=1,\n",")"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["Now that the model is trained, you are ready to start using it to classify images.\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"]},{"cell_type":"code","execution_count":20,"metadata":{"button":false,"collapsed":true,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"outputs":[],"source":["model.save('classifier_resnet_model.h5')"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\vadim\\AppData\\Local\\Temp\\ipykernel_1232\\1642749776.py:1: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n","  predict = model.predict_generator(testing_generator,len(testing_generator))\n"]}],"source":["predict = model.predict_generator(testing_generator,len(testing_generator))"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/plain":["array([[1.15534436e-04, 9.99884427e-01],\n","       [9.92536485e-01, 7.46344728e-03],\n","       [5.90149762e-10, 9.99999940e-01],\n","       ...,\n","       [9.99910951e-01, 8.89905859e-05],\n","       [9.99692798e-01, 3.07279610e-04],\n","       [9.97818589e-01, 2.18145829e-03]], dtype=float32)"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["predict"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane.\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["### Thank you for completing this lab!\n","\n","This notebook was created by Alex Aklson. I hope you found this lab interesting and educational.\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3\\_LAB1).\n"]},{"cell_type":"markdown","metadata":{},"source":["## Change Log\n","\n","| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                          |\n","| ----------------- | ------- | ---------- | ----------------------------------------------------------- |\n","| 2020-09-18        | 2.0     | Shubham    | Migrated Lab to Markdown and added to course repo in GitLab |\n"]},{"cell_type":"markdown","metadata":{"button":false,"deletable":true,"new_sheet":false,"run_control":{"read_only":false}},"source":["<hr>\n","\n","Copyright © 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_medium=dswb&utm_source=bducopyrightlink&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork20647850-2022-01-01).\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.13 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"d6569922b3472e3b6d4e09d229bfd43cb8d5bc7f4f96724c1235498c63839d60"}}},"nbformat":4,"nbformat_minor":4}
